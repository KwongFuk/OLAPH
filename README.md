# OLAPH
OLAPH: Improving Factuality in Biomedical Long-form Question Answering

This is a repository for [OLAPH: Improving Factuality in Biomedical Long-form Question Answering]() by .

[MedLFQA]() | [OLAPH Models]() | [Summary]() | [Paper]() 

1) **MedLFQA** is a reconstructed format of long-form question-answering (LFQA) benchmark datasets in biomedical domain to facilitate automatic evaluation.
2) **OLAPH** is a framework that reduces hallucinations and includes crucial claims by utilizing automatic evaluation to select the best response in sampling predictions and designing to answer questions in preferred manner.

## Content
1. [Installation](#installation)
2. [Quick Usage](#quick-usage)
3. [Datasets](#datasets)
4. [Training](#training)
5. [Inference](#inference)
6. [Iterative Learning](#iterative-learning)
7. [FactScore](#factscore)
8. [FAQ](#faq)
9. [Citation](#citation)
10. [Contact Information](#contact-information)

## Installation
Please create a conda environment by running the command below.
Note that we use two different environments to train and inference.
I will ensure that everything is integrated into a single environment and functions properly in the future.

For training,
```
conda env create -f training.yaml
conda activate olaph_training
```

For inference,
```
conda env create -f inference.yaml
conda activate olaph_inference
```


## Quick Usage
You can download 7B models trained with our OLAPH framework from HuggingFace hub.
```py
from transformers import AutoTokenizer, AutoModelForCausalLM

model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16).to(device)
tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side="left")

query = "Alright so I don't know much about Lexapro would you tell me ore about it?"

input_ids = tokenizer.encode(query, return_tensors="pt").to(device)
output = model.generate(input_ids, max_length=512, no_repeat_ngram_size=2, do_sample=False, top_p=1.0).to(device)
response = tokenizer.decode(output[0], skip_special_tokens=True).strip()

print ("Model prediction: ", response)
```

## Datasets
**MedLFQA** is a reconstructed format of long-form question-answering (LFQA) benchmark datasets in biomedical domain to facilitate automatic evaluation.
We construct the **MedLFQA** with four biomedical LFQA benchmark datasets: [LiveQA](https://github.com/abachaa/LiveQA_MedicalTask_TREC2017), [MedicationQA](https://github.com/abachaa/Medication_QA_MedInfo2019), [HealthSearchQA](https://huggingface.co/datasets/katielink/healthsearchqa), and [K-QA](https://github.com/Itaymanes/K-QA).
Our **MedLFQA** instance is comprised of four components: question (Q), long-form answer (A), Must Have Statements (MH), Nice to Have Statements (NH).
We provide the reconstructed datasets for automatic evaluation of long-form generated responses.

## Inference

* Sampling Predictions (Including Automatic Evaluation)

```
# For first sampling predictions
conda activate olaph_inference

export DATANAME=live_qa
export HUGGINGFACE_MODEL_DIR=dmis-lab/selfbiorag_7b
CUDA_VISIBLE_DEVICES=0 python pdata_collection.py \
--model_name_or_path ${HUGGINGFACE_MODEL_DIR} \
--eval_data ${DATANAME} \
```

```
# Sampling prediction during Iterative learning (i.e., after SFT or DPO)
conda deactivate
conda activate olaph_inference

export DATANAME=live_qa
export HUGGINGFACE_MODEL_DIR=your_trained_model
CUDA_VISIBLE_DEVICES=0 python pdata_collection.py \
--model_name_or_path ${HUGGINGFACE_MODEL_DIR} \
--eval_data ${DATANAME} \
```


## Training

* Supervised Fine-Tuning (SFT)

After we obtain sampled predictions from previous step, we use SFT to recognize the question-answering task.
Rather than training on human-annotated answer or pseudo-optimal responses generated by GPT-4, we set a self-generated response as a labeled asnwer to remove the depedency on resources in annotation datasets.
We use a representative 7B model for Self-BioRAG.
If you want to use another models with difference configuration, you should change directions of recipes.

```
conda activate olaph_training
cd alignment-handbook
```

```
CUDA_VISIBLE_DEVICES=0,1,2,3 ACCELERATE_LOG_LEVEL=info accelerate launch \
--config_file recipes/accelerate_configs/deepspeed_zero3.yaml  \
--num_processes 4 \
scripts/run_sft.py \
recipes/selfbiorag_7b/sft/config_full.yaml \
```

* Direct Preference Optimization (DPO)

```
conda activate olaph_training
cd alignment-handbook
```

```
CUDA_VISIBLE_DEVICES=0,1,2,3 ACCELERATE_LOG_LEVEL=info accelerate launch \
--config_file recipes/accelerate_configs/deepspeed_zero3.yaml  \
--num_processes 4 \
scripts/run_dpo.py \
recipes/selfbiorag_7b/sft/config_full.yaml \
```

## Iterative Learning
We train and generate sampling predictions through separate files and do several times.
In future, we will provide the processes execution in one simple bash file.

Our iterative learning consists of the following processes \
Sampling predictions (Raw) - SFT - Sampling predictions - DPO - DPO - DPO (until convergence)

## FactScore
TBA

## FAQ
TBA

## Citation
TBA

## Contact Information
TBA